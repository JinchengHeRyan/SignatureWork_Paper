%! Author = jincheng
%! Date = 2022/4/24


\section{Background}\label{sec:background}

In this section, we give a detailed background about what is target speaker separation (TSS) task, what kinds of metrics can be used to define the performance, what kinds of techniques existed can be used to do TSS task.

\subsection{What is TSS task}\label{subsec:what-is-tss-task}

Assume there are $N$ speakers talking simultaneously, they are called $s_1, s_2, \dots s_N$. The waveform of speech utterance for each speaker is $U_i\left( t \right)$ while $i\in\left[ 1, N \right]$. Assume the noise is $e\left( t \right)$.
So the mixed utterance $X\left( t \right)$ is as equation~\ref{eq:mixed}.
\begin{equation}
    X\left( t \right) = \sum\limits_{i=1}^N U_i\left( t \right) + e\left( t \right)
    \label{eq:mixed}
\end{equation}
One waveform example is as figure~\ref{fig:waveform}.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{img/waveform}
    \caption{One example of how waveform looks like}
    \label{fig:waveform}
\end{figure}

For the mission of Target Speaker Separation (TSS) Task. Assume we only need to extract the voice of speaker $s_n$ from $X\left( t \right)$ and we have the reference utterance from $s_n$, denoted as $R_n$.
The task is given $X\left( t \right) = \sum\limits_{i=1}^N U_i\left( t \right) + e\left( t \right)$ and $R_n$, we need to get $U_n\left( t \right)$.

\subsection{Metrics}\label{subsec:metrics}

Using meaningful metrics to define the performance of the separation system is important.
To evaluate the performance of separation system, the metric should be designed to objectively represent ``how good'' the extracted waveform is.
The idea of projecting one high dimensional vector onto another high dimensional vector comes in mind.

\begin{itemize}
    \item source-to-distortion ratio (SDR)

    SDR is defined in~\cite{performance_measurement}, and is as equation~\ref{eq:sdr}.
    \begin{equation}
        \text{SDR} \coloneqq 10\log_{10}\frac{\| s_{\text{target}} \|^2}{\| e_{\text{interf}} + e_{\text{noise}} + e_{\text{artif}} \|^2}
        \label{eq:sdr}
    \end{equation}

    \item scale-invariant source-to-noise ratio (SI-SNR)

    SI-SNR is proposed in~\cite{tasnet}, and is as equation~\ref{eq:si-snr}.
    \begin{equation}
        \begin{aligned}
            s_{\text{target}} &= \frac{\left< \hat{s}, s \right>s}{\| s \|^2} \\
            e_{\text{noise}} &= \hat{s} - s_{\text{target}} \\
            \text{SI-SNR} &\coloneqq 10\log_{10}\frac{\| s_{\text{target}} \|^2}{\| e_{\text{noise}} \|^2}
        \end{aligned}\label{eq:si-snr}
    \end{equation}
\end{itemize}

\subsection{Methodology}\label{subsec:methodology}

To extract target speaker's voice from the mixed utterance, we first need to extract the information of the target speaker.
One common way is to use a vector to represent target speaker's information, and we call it the speaker embedding.
There are many ways and many different speaker embeddings, such as i-vector in~\cite{i-vector}, a speaker embedding based on end-to-end neural network in~\cite{deep_speaker}, x-vector in~\cite{x-vector}, d-vector in~\cite{d-vector}.
As Transformer architecture~\cite{attention} being existed, there are some Transformer encoder based speaker embeddings happen, such as s-vector in~\cite{s-vector}.

After extracting the speaker embedding, there are basically two different ways to do the separation task, one is frequency-domain methods, another is time-domain methods.

\subsubsection{Frequency-domain methods}

For the frequency-domain methods, the central part is processing the spectrogram. Here gives an example in figure~\ref{fig:spec_exp}.
\begin{figure}
    \centering
    \stackunder[5pt]{\includegraphics[width=0.9\linewidth]{img/spec_exp_mix}}{(a)}
    \stackunder[5pt]{\includegraphics[width=0.9\linewidth]{img/spec_exp_tar}}{(b)}
    \stackunder[5pt]{\includegraphics[width=0.9\linewidth]{img/spec_exp_est}}{(c)}
    \caption{Spectrogram example of 1650-167613-0006. (a) Spectrogram of the mixed utterance; (b) Spectrogram of the target speaker utterance; (c) Spectrogram of the estimated target speaker utterance. }
    \label{fig:spec_exp}
\end{figure}

\subsubsection{Time-domain methods}
