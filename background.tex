%! Author = jincheng
%! Date = 2022/4/24


\section{Background}\label{sec:background}

In this section, we give a detailed background about what is target speaker separation (TSS) task, what kinds of metrics can be used to define the performance, what kinds of techniques existed can be used to do TSS task.

\subsection{What is TSS task}\label{subsec:what-is-tss-task}

Assume there are $N$ speakers talking simultaneously, they are called $s_1, s_2, \dots s_N$. The waveform of speech utterance for each speaker is $U_i\left( t \right)$ while $i\in\left[ 1, N \right]$. Assume the noise is $e\left( t \right)$.
So the mixed utterance $X\left( t \right)$ is as equation~\ref{eq:mixed}.
\begin{equation}
    X\left( t \right) = \sum\limits_{i=1}^N U_i\left( t \right) + e\left( t \right)
    \label{eq:mixed}
\end{equation}
One waveform example is as figure~\ref{fig:waveform}.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{img/waveform}
    \caption{One example of how waveform looks like}
    \label{fig:waveform}
\end{figure}

For the mission of Target Speaker Separation (TSS) Task. Assume we only need to extract the voice of speaker $s_n$ from $X\left( t \right)$ and we have the reference utterance from $s_n$, denoted as $R_n$.
The task is given $X\left( t \right) = \sum\limits_{i=1}^N U_i\left( t \right) + e\left( t \right)$ and $R_n$, we need to get $U_n\left( t \right)$.

\subsection{Metrics}\label{subsec:metrics}

Using meaningful metrics to define the performance of the separation system is important.
To evaluate the performance of separation system, the metric should be designed to objectively represent ``how good'' the extracted waveform is.
The idea of projecting one high dimensional vector onto another high dimensional vector comes in mind.

\begin{itemize}
    \item source-to-distortion ratio (SDR)

    SDR is defined in~\cite{performance_measurement}, and is as equation~\ref{eq:sdr}.
    \begin{equation}
        \text{SDR} \coloneqq 10\log_{10}\frac{\| s_{\text{target}} \|^2}{\| e_{\text{interf}} + e_{\text{noise}} + e_{\text{artif}} \|^2}
        \label{eq:sdr}
    \end{equation}

    \item scale-invariant source-to-noise ratio (SI-SNR)

    SI-SNR is proposed in~\cite{tasnet}, and is as equation~\ref{eq:si-snr}.
    \begin{equation}
        \begin{aligned}
            s_{\text{target}} &= \frac{\left< \hat{s}, s \right>s}{\| s \|^2} \\
            e_{\text{noise}} &= \hat{s} - s_{\text{target}} \\
            \text{SI-SNR} &\coloneqq 10\log_{10}\frac{\| s_{\text{target}} \|^2}{\| e_{\text{noise}} \|^2}
        \end{aligned}\label{eq:si-snr}
    \end{equation}
\end{itemize}

\subsection{Methodology}\label{subsec:methodology}

To extract target speaker's voice from the mixed utterance, we first need to extract the information of the target speaker.
One common way is to use a vector to represent target speaker's information, and we call it the speaker embedding.
There are many ways and many different speaker embeddings, such as i-vector based on simple factor analysis in~\cite{i-vector}, a speaker embedding based on end-to-end neural network in~\cite{deep_speaker}, x-vector in~\cite{x-vector}, d-vector in~\cite{d-vector}.
As Transformer architecture~\cite{attention} being existed, there are some Transformer encoder based speaker embeddings happen, such as s-vector in~\cite{s-vector}.

After extracting the speaker embedding, there are basically two different ways to do the separation task, one is frequency-domain methods, another is time-domain methods.

\subsubsection{Frequency-domain methods}

For the frequency-domain methods, the central part is processing the magnitude spectrogram. Here gives an example in figure~\ref{fig:spec_exp}.
\begin{figure}
    \centering
    \stackunder[5pt]{\includegraphics[width=0.9\linewidth]{img/spec_exp_mix}}{(a)}
    \stackunder[5pt]{\includegraphics[width=0.9\linewidth]{img/spec_exp_tar}}{(b)}
    \stackunder[5pt]{\includegraphics[width=0.9\linewidth]{img/spec_exp_est}}{(c)}
    \caption{Magnitude spectrogram example of 1650-167613-0006. (a) Magnitude spectrogram of the mixed utterance; (b) Magnitude spectrogram of the target speaker utterance; (c) Magnitude spectrogram of the estimated target speaker utterance. }
    \label{fig:spec_exp}
\end{figure}

For each utterance, by doing Fourier transform, it can be converted into magnitude spectrogram and phase spectrogram.
For the frequency-domain methods, the central part is to mask the magnitude spectrogram.
In (a), it is the magnitude spectrogram of the mixed utterance which contains two speakers, and (b) is the magnitude spectrogram of the target speaker's utterance.
The goal is by given (a) and the target speaker's reference audio, we want to get (b). (c) is our system's estimated magnitude spectrogram of target speaker's utterance, and it is very visually similar to (b).
For the frequency-domain methods, the mask is estimated, and dot multiplied to the mixed magnitude spectrogram for getting the estimated one.
The whole process can be described in figure~\ref{fig:sep_process}.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{img/sep_process}
    \caption{Frequency-domain methods general description}
    \label{fig:sep_process}
\end{figure}

There are many methods using this framework, such as deep clustering~\cite{deep_clustering}, which trains a deep network to produce spectrogram embeddings.
The estimated mask is binary, and the projection process is achieved by the deep neural network, and the objective function it trains is as equation~\ref{eq:deep_clustering}.
\begin{equation}
    C_Y\left( V \right) = \| VV^T - YY^T \|_F^2
    \label{eq:deep_clustering}
\end{equation}

Also, permutation invariant training (PIT) in~\cite{PIT}, which is widely used in blind speaker separation.
One big problem of speaker independent speech separation is the permutation of the output of the network.
PIT solves this problem, which outputs all the permutations and using the smallest mean squared error (MSE) as the optimization goal.
PIT structure is as figure~\ref{fig:PIT_structure}.
To better solve the issue of tracking the speaking person, one utterance-level algorithm uPIT in~\cite{uPIT} is proposed.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{img/PIT_structure}
    \caption{Permutation invariant training structure}
    \label{fig:PIT_structure}
\end{figure}

To better solve the disadvantages of deep clustering, a new deep embedding clustering based algorithm DEF-DL~\cite{DEF-DL} is proposed.
DEF-DL uses supervised network to replace of K-means clustering algorithm, and use this feature as the input of uPIT network to separate speech.
The whole structure is as figure~\ref{fig:def_dl_structure}.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{img/DEF-DL_structure}
    \caption{DEF-DL structure}
    \label{fig:def_dl_structure}
\end{figure}

\subsubsection{Time-domain methods}
